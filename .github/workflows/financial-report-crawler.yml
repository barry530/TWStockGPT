name: Financial Report Crawler
run-name: Crawler Demo Action

# trigger time
on:
  schedule: 
    - cron: "0 23 16 * *"  # UTC -> At 23:00 on the 16th day of every month
  workflow_dispatch:  # allow manual action

# Define the action
jobs:
  monthly-revenue-crawler:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"
      - name: Install Python Dependencies
        run: pip install pandas requests lxml google google-api-python-client
      - name: Use GOOGLE_DRIVE_CREDENTIAL_API_KEY
        env:
            GOOGLE_CREDENTIALS: ${{ secrets.GOOGLE_DRIVE_CREDENTIAL_API_KEY }}
        run: echo "$GOOGLE_DRIVE_CREDENTIAL_API_KEY" > credentials.json
      - name: Run the Crawler Script
        run: python ./src/web_crawler/main_crawler.py
      - name: Commit Data Back to Github Repo
        run: |
          git config --global user.name "monthly-revenue-crawler"
          git config --global user.email "monthly-revenue-crawler@gmail.com"
          git add . && git commit -m "financial report crawler"
          git push origin main
      - name: End
        run: echo 'Done!'
  
